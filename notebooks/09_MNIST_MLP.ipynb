{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09_MNIST_MLP.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_NZwOwd9KsKz"},"source":["# MNISTを用いた文字認識（MLP）\n","\n","\n","---\n","## 目的\n","多層パーセプトロン（MLP）を用いてMNISTデータセットに対する文字認識を行う．\n","評価はConfusion Matrixにより各クラスの認識率を用いて行う．"]},{"cell_type":"markdown","metadata":{"id":"H-0vTan1NYLI"},"source":["## 使用するデータセット\n","今回の文字認識では，MNIST Datasetを用いる．[MNIST Dataset](http://yann.lecun.com/exdb/mnist/)は，0から9までの数字が記述されている画像から構成されたデータセットである．MNIST Datasetの文字画像は，以下のように白黒で比較的認識しやすいように画像処理されている．\n","\n","![MNIST_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/143078/559938dc-9a99-d426-010b-e000bca0aac6.png)"]},{"cell_type":"markdown","metadata":{"id":"RsGSLNkYQmkG"},"source":["## モジュールのインポート\n","はじめに必要なモジュールをインポートする．\n","\n","今回は`torch` (PyTorch) をインポートする．"]},{"cell_type":"code","metadata":{"id":"SLeGt2xaNFOB"},"source":["from time import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torchsummary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ue60y-upamyo"},"source":["## データセットの読み込みと確認\n","学習データ（MNIST Dataset）を読み込みます．\n","\n","読み込んだ学習データのサイズを確認します．\n","学習データ数は6万枚，評価データは1万枚，1つのデータのサイズは28x28の786次元となっています．"]},{"cell_type":"code","metadata":{"id":"n7zpMk-4axYm"},"source":["train_data = torchvision.datasets.MNIST(root=\"./\", train=True, transform=transforms.ToTensor(), download=True)\n","test_data = torchvision.datasets.MNIST(root=\"./\", train=False, transform=transforms.ToTensor(), download=True)\n","\n","print(type(train_data.data), type(train_data.targets))\n","print(type(test_data.data), type(test_data.targets))\n","print(train_data.data.size(), train_data.targets.size())\n","print(test_data.data.size(), test_data.targets.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MN-KoymJbe25"},"source":["### MNISTデータセットの表示\n","\n","MNISTデータセットに含まれる画像を表示してみます．\n","ここでは，matplotlibを用いて複数の画像を表示させるプログラムを利用します．"]},{"cell_type":"code","metadata":{"id":"ehg-aZh8be9Z"},"source":["import matplotlib.pyplot as plt\n","\n","cols = 10\n","\n","plt.clf()\n","fig = plt.figure(figsize=(14, 1.4))\n","for c in range(cols):\n","    ax = fig.add_subplot(1, cols, c + 1)\n","    ax.imshow(train_data[c][0].view(28, 28), cmap=plt.get_cmap('gray'))\n","    ax.set_axis_off()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G418kZOgToXR"},"source":["## ネットワークモデルの定義\n","\n","ニューラルネットワークを定義します．\n","ここでは，入力層，中間層，出力層から構成される３層のニューラルネットワークとします．\n","\n","入力層のユニット数は入力データのサイズによります．\n","ここでは`28 x 28 = 786`とし，画像の画素値を1次元配列として並べ替えたデータを入力するように指定します．\n","\n","中間層と出力層のユニット数は引数として与え，それぞれ`n_hidden`，`n_out`とします．\n","PyTorchでは，`__init__`関数にこれらの引数を与えて各層を定義します．\n","各層はLinear関数としています．これは全結合層を意味しています．\n","また，`self.act`で活性化関数を指定します．ここでは，シグモイド関数を活性化関数として指定します．\n","\n","そして，`forward`関数で定義した層を接続して処理するように記述します．\n","`forward`関数の引数`x`は入力データを示しています．\n","それを`forward`関数で定義した`l1`という中間層および活性化関数`act`へ順番に入力します．\n","その出力を`h1`としています．\n","`h1`は出力層`l2`に与えられ，その出力を`h2`としています．"]},{"cell_type":"code","metadata":{"id":"8FJhkBJnTuPd"},"source":["class MLP(nn.Module):\n","    def __init__(self, n_hidden, n_out):\n","        super().__init__()\n","        self.l1 = nn.Linear(28*28, n_hidden)\n","        self.l2 = nn.Linear(n_hidden, n_out)\n","        self.act = nn.Sigmoid()\n","        \n","    def forward(self, x):\n","        h1 = self.act(self.l1(x))\n","        h2 = self.l2(h1)\n","        return h2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OF_0s3vBYBES"},"source":["## ネットワークの作成\n","上のプログラムで定義したネットワークを作成します．\n","\n","まず，中間層と出力層のユニット数を定義します．\n","ここでは，中間層のユニット数`hidden_num`を16，出力層のユニット数`out_num`をMNISTのクラス数に対応する10とします．\n","\n","各層のユニット数を上で定義した`MLP`クラスの引数として与え，ネットワークモデルを定義します．\n","\n","学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．\n","また，学習率を0.01，モーメンタムを0.9として引数に与えます．\n","\n","最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．第一引数に詳細を表示したいモデル，第二引数にネットワークへ入力されるデータのサイズを指定します．\n","これによって，ネットワークの構造を確認することができます．"]},{"cell_type":"code","metadata":{"id":"FTAUhy9qX4QU"},"source":["# ユニット数の定義\n","hidden_num = 16\n","out_num = 10\n","\n","# ネットワークの作成\n","model = MLP(n_hidden=hidden_num, n_out=out_num)\n","\n","# 最適化手法の設定\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# 定義したモデルの情報を表示\n","torchsummary.summary(model, (1, 28*28), device='cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iGfy76HRYy4S"},"source":["## 学習\n","読み込んだMNISTデータセットと作成したネットワークを用いて，学習を行います．\n","\n","1回の誤差を算出するデータ数（ミニバッチサイズ）を100，学習エポック数を10とします．\n","\n","次にデータローダーを定義します．\n","データローダーでは，上で読み込んだデータセット（`train_data`）を用いて，for文で指定したミニバッチサイズでデータを読み込むオブジェクトを作成します．\n","この時，`shuffle=True`と設定することで，読み込むデータを毎回ランダムに指定します．\n","\n","次に，誤差関数を設定します．\n","今回は，分類問題をあつかうため，クロスエントロピー誤差を計算するための`CrossEntropyLoss`を`criterion`として定義します．\n","\n","学習を開始します．\n","\n","各更新において，学習用データと教師データをそれぞれ`image`と`label`とします．\n","学習モデルにimageを与えて各クラスの確率yを取得します．\n","各クラスの確率yと教師ラベルtとの誤差を`criterion`で算出します．\n","また，認識精度も算出します．\n","そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．"]},{"cell_type":"code","metadata":{"id":"C0iI0zC-ZSY2"},"source":["# ミニバッチサイズ・エポック数の設定\n","batch_size = 100\n","epoch_num = 10\n","\n","# データローダーの設定\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","# 誤差関数の設定\n","criterion = nn.CrossEntropyLoss()\n","\n","# ネットワークを学習モードへ変更\n","model.train()\n","\n","# 学習の実行\n","for epoch in range(1, epoch_num+1):\n","    sum_loss = 0.0\n","    count = 0\n","    \n","    for image, label in train_loader:\n","        image = image.view(image.size()[0], -1)\n","        y = model(image)\n","        \n","        loss = criterion(y, label)\n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        sum_loss += loss.item()\n","        \n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","\n","    print(\"epoch:{}, mean loss: {}, mean accuracy: {}\".format(epoch, sum_loss/600, count.item()/60000.))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ti1LytKAZYIO"},"source":["## テスト\n","\n","学習したネットワークを用いて，テストデータに対する認識率の確認を行います．\n","\n","`model.eval()`を適用することで，ネットワーク演算を評価モードへ変更します．\n","これにより，学習時と評価時で挙動が異なる演算（dropout等）を変更することが可能です．\n","また，`torch.no_grad()`を適用することで，学習時には必要になる勾配情報を保持することなく演算を行います．"]},{"cell_type":"code","metadata":{"id":"635DQ0ATYBJN"},"source":["# データローダーの準備\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n","\n","# ネットワークを評価モードへ変更\n","model.eval()\n","\n","# 評価の実行\n","count = 0\n","with torch.no_grad():\n","    for image, label in test_loader:\n","        image = image.view(image.size()[0], -1)\n","        y = model(image)\n","\n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","\n","print(\"test accuracy: {}\".format(count.item() / 10000.))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7YT4hqE3Ycpg"},"source":["## 課題\n","\n","### 1. ネットワークの構造を変更し，認識精度の変化を確認しましょう．\n","\n","**ヒント：ネットワーク構造の変更としては，次のようなものが考えられます．**\n","* 中間層のユニット数\n","* 層の数\n","* 活性化関数\n","  * `nn.Tanh()`や`nn.ReLU()`, `nn.LeakyReLU()`などが考えられます．\n","  * その他のPyTorchで使用できる活性化関数は[こちらページ](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)にまとめられています．\n","\n","※ ネットワーク構造を変更した際には，`torchsummary.summary()`関数を使用し，ネットワーク構造を変更した際のパラメータ数の変化を確認してみましょう．\n","\n","  \n"]}]}