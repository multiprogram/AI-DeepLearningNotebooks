{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"16_cartpole_dqn.ipynb","provenance":[{"file_id":"1Y-JU6S-lB5KG326EGKJxDxfISYP-ByyR","timestamp":1627257977724}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rMvOfX_2IuUo"},"source":["# 深層強化学習によるCart Pole制御\n","\n","---\n","## 目的\n","深層強化学習を用いてCart Pole制御を行う．\n","ここで，Cart Pole制御とは台車に乗っている棒が倒れないように台車を左右に動かすことである．\n","\n","Q-Tableを用いた従来のQ学習による方法とQ-Networkを用いた方法の２種類を行う．\n"]},{"cell_type":"markdown","metadata":{"id":"Log6bIaiDnkV"},"source":["## 準備\n","\n","### Google Colaboratoryの設定確認・変更\n","本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n","**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n","\n","\n","### モジュールの追加インストール\n","下記のプログラムを実行して，実験結果の表示に必要な追加ライブラリやモジュールをインストールする．"]},{"cell_type":"code","metadata":{"id":"piC8yNcqDmyC"},"source":["!apt-get -qq -y install libcusparse9.1 libnvrtc9.1 libnvtoolsext1 > /dev/null\n","!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.9.1 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n","!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n","\n","!pip -q install gym\n","!pip -q install pyglet\n","!pip -q install pyopengl\n","!pip -q install pyvirtualdisplay"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2aKH_oZ9Lb6i"},"source":["## モジュールのインポート\n","はじめに必要なモジュールをインポートする．\n","\n","今回はPyTorchに加えて，Cart Poleを実行するためのシミュレータであるopenAI Gym（gym）をインポートする．"]},{"cell_type":"code","metadata":{"id":"z8rIC4r6LFte"},"source":["import numpy as np\n","import gym\n","import gym.spaces\n","\n","import time\n","import math\n","import random\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import collections\n","from itertools import count\n","from PIL import Image\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","# 使用するデバイス（GPU or CPU）の決定\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Use device:\", device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0V-KflRLV-6"},"source":["# Q-Learning (Q-table)\n","Q-Learningとは、強化学習の手法の中で、TD法を用いた代表的な手法の一つです。最適な行動価値(Q値)を推定するように学習を行いQ値を行動の指針として用いることで最適な行動を行います。Q学習では，全ての状態$s$と行動$a$に対する価値$Q(s, a)$を記録するテーブル(Q-table) を作成します。しかし、初期段階では、各状態と行動に対する正確な行動価値がわからないため、Q-tableを任意の値に初期化します。その後、あらゆる状態の下で行動を行い推定した行動価値を用いてQ-tableを修正していきます。以下に簡略化したQ-Learningの学習方法を記載します。\n","1. ある環境における全ての状態と行動に対する価値(Q値)を記録するためのQ-tableを作成\n","2. Q-tableに記録されたQ値を任意の値で初期化\n","3. $\\epsilon$-greedy法などを用いて環境に対する行動を選択\n","4. 行った行動に対する報酬値とQ-tableに記録されたQ値をもとにQ-tableを更新\n","5. 最適なQ-tableが完成するまで3,4を繰り返す"]},{"cell_type":"markdown","metadata":{"id":"0mdO4rIl6IUQ"},"source":["## 環境の作成\n","今回の実験で使用する環境の作成を行います。 [OpenAI Gym](https://github.com/openai/gym) は、様々な種類の環境を提供しているモジュールです。\n"," \n","今回の実験ではgymで利用できるCartPoleを実行します。\n","まず、gym.make関数で実行したい環境を指定します。\n","その後、reset関数を実行することで、環境を初期化します。また、環境に対する情報を表示することもできます。\\\n","CartPoleは、Cartを操作し、Cartに乗ったPoleを倒さないようにするという環境です。環境における行動は右と左に動くという離散的な行動をもち、状態は連続的な値であらわされたCart位置、Cart速度、Poleの角度、Poleの角速度からなる状態をもっています。"]},{"cell_type":"code","metadata":{"id":"_a46O-q36IUr"},"source":["env = gym.make('CartPole-v0')\n","\n","obs = env.reset()\n","print('observation space:',env.observation_space)\n","print('action space:',env.action_space)\n","print('initial observation',obs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hnmhZNvQLw4u"},"source":["## Q-tableの作成、離散化処理\n","Q-tableの作成とCartPole環境における状態の離散化処理を行います。\\\n","Q-LearningはQ-tableを用いた学習を行います。しかし、今回使用する環境であるCartPoleはCart位置、Cart速度、Poleの角度、Poleの角速度からなる4次元の状態をもっておりCart速度は(-2.4～2.4)、Cart速度は(-3.0～3.0)、Poleの角度は(-0.5, 0.5)、Poleの角速度は(-2.0, 2.0)の範囲で連続的な数値となっています。Q-Learningでは、任意の大きさのQ-tableを作成しなければいけないため、連続的な数値ではQ-tableを作成することができません。なので、状態を分割し離散的な値に変換することでQ-tableを作成可能とします。\n","今回はnumpyのdigitize関数とlinspace関数を組み合わせて離散化処理を行います。まず、linspace関数で分割数に応じて状態の範囲を区切ります。そして、dizitize関数である値が区切られた範囲でどこの区画に属するのかを返します。これにより、連続的な数値であってもその値がどの区画なのかという数値に変換されるため、離散化された値とすることができます。\n","状態の分割により環境における状態数が決定されるため決定した状態数の大きさに合わせたQ-tableを作成します。\\\n","初めに決定した状態の分割数$x$により、離散化する際の値が変化し、Q-tableの大きさが変化します。CartPoleは4次元の状態を持っているためQ-tableの大きさは $x^4(状態数)×2(actionの数)$となります。"]},{"cell_type":"code