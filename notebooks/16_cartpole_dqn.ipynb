{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"16_cartpole_dqn.ipynb","provenance":[{"file_id":"1Y-JU6S-lB5KG326EGKJxDxfISYP-ByyR","timestamp":1627257977724}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rMvOfX_2IuUo"},"source":["# 深層強化学習によるCart Pole制御\n","\n","---\n","## 目的\n","深層強化学習を用いてCart Pole制御を行う．\n","ここで，Cart Pole制御とは台車に乗っている棒が倒れないように台車を左右に動かすことである．\n","\n","Q-Tableを用いた従来のQ学習による方法とQ-Networkを用いた方法の２種類を行う．\n"]},{"cell_type":"markdown","metadata":{"id":"Log6bIaiDnkV"},"source":["## 準備\n","\n","### Google Colaboratoryの設定確認・変更\n","本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n","**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n","\n","\n","### モジュールの追加インストール\n","下記のプログラムを実行して，実験結果の表示に必要な追加ライブラリやモジュールをインストールする．"]},{"cell_type":"code","metadata":{"id":"piC8yNcqDmyC"},"source":["!apt-get -qq -y install libcusparse9.1 libnvrtc9.1 libnvtoolsext1 > /dev/null\n","!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.9.1 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n","!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n","\n","!pip -q install gym\n","!pip -q install pyglet\n","!pip -q install pyopengl\n","!pip -q install pyvirtualdisplay"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2aKH_oZ9Lb6i"},"source":["## モジュールのインポート\n","はじめに必要なモジュールをインポートする．\n","\n","今回はPyTorchに加えて，Cart Poleを実行するためのシミュレータであるopenAI Gym（gym）をインポートする．"]},{"cell_type":"code","metadata":{"id":"z8rIC4r6LFte"},"source":["import numpy as np\n","import gym\n","import gym.spaces\n","\n","import time\n","import math\n","import random\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import collections\n","from itertools import count\n","from PIL import Image\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","# 使用するデバイス（GPU or CPU）の決定\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Use device:\", device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0V-KflRLV-6"},"source":["# Q-Learning (Q-table)\n","Q-Learningとは、強化学習の手法の中で、TD法を用いた代表的な手法の一つです。最適な行動価値(Q値)を推定するように学習を行いQ値を行動の指針として用いることで最適な行動を行います。Q学習では，全ての状態$s$と行動$a$に対する価値$Q(s, a)$を記録するテーブル(Q-table) を作成します。しかし、初期段階では、各状態と行動に対する正確な行動価値がわからないため、Q-tableを任意の値に初期化します。その後、あらゆる状態の下で行動を行い推定した行動価値を用いてQ-tableを修正していきます。以下に簡略化したQ-Learningの学習方法を記載します。\n","1. ある環境における全ての状態と行動に対する価値(Q値)を記録するためのQ-tableを作成\n","2. Q-tableに記録されたQ値を任意の値で初期化\n","3. $\\epsilon$-greedy法などを用いて環境に対する行動を選択\n","4. 行った行動に対する報酬値とQ-tableに記録されたQ値をもとにQ-tableを更新\n","5. 最適なQ-tableが完成するまで3,4を繰り返す"]},{"cell_type":"markdown","metadata":{"id":"0mdO4rIl6IUQ"},"source":["## 環境の作成\n","今回の実験で使用する環境の作成を行います。 [OpenAI Gym](https://github.com/openai/gym) は、様々な種類の環境を提供しているモジュールです。\n"," \n","今回の実験ではgymで利用できるCartPoleを実行します。\n","まず、gym.make関数で実行したい環境を指定します。\n","その後、reset関数を実行することで、環境を初期化します。また、環境に対する情報を表示することもできます。\\\n","CartPoleは、Cartを操作し、Cartに乗ったPoleを倒さないようにするという環境です。環境における行動は右と左に動くという離散的な行動をもち、状態は連