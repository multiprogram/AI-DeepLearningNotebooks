{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"11_CIFAR_CNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wJU2RPpSvlQT"},"source":["# CIFAR10を用いた物体認識\n","\n","\n","---\n","## 目的\n","CIFAR10 Datasetを用いて10クラスの物体認識を行う．プログラムの構成は，MNISTによる文字認識のプログラムと同様になっているため，基礎的な説明はそちらを参照して頂きたい．このページでは，MNISTによる文字認識のプログラムとの差分について書いていく．\n","\n","GPUを用いたネットワークの計算を行う．\n"]},{"cell_type":"markdown","metadata":{"id":"5rQGfxWYK_4O"},"source":["## 準備\n","\n","### Google Colaboratoryの設定確認・変更\n","本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n","**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n"]},{"cell_type":"markdown","metadata":{"id":"C2tsYagqvloK"},"source":["## 使用するデータセット\n","\n","### データセット\n","今回の物体認識では，CIFAR10データセットを用いる．CIFAR10データセットは，飛行機や犬などの10クラスの物体が表示されている画像から構成されたデータセットである．\n","\n","![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)"]},{"cell_type":"markdown","metadata":{"id":"Xo4jjpmwvle1"},"source":["## モジュールのインポート\n","はじめに必要なモジュールをインポートする．\n","\n","### GPUの確認\n","GPUを使用した計算が可能かどうかを確認します．\n","\n","`Use CUDA: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n","Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．\n"]},{"cell_type":"code","metadata":{"id":"iCeaCulfvlao"},"source":["# モジュールのインポート\n","from time import time\n","import torch\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torchsummary\n","\n","# GPUの確認\n","use_cuda = torch.cuda.is_available()\n","print('Use CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ppjeW5MbysXC"},"source":["## データセットの読み込みと確認\n","学習データ（CIFAR10データセット）を読み込みます．\n","\n","読み込んだ学習データのサイズを確認します．\n","学習データは5万枚，1つのデータサイズは3x32x32の画像のような形式となっています．\n","これは32x32ピクセルのカラー画像という意味になります．"]},{"cell_type":"code","metadata":{"id":"K_xx-TkVvls6"},"source":["train_data = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=transforms.ToTensor(), download=True)\n","test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False, transform=transforms.ToTensor(), download=True)\n","\n","print(train_data)\n","print(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LKi4gTk8vlxe"},"source":["### CIFAR10データセットの表示\n","CIFAR10データセットに含まれる画像を表示してみます．\n","ここでは，matplotlibを用いて複数の画像を表示させるプログラムを利用します．"]},{"cell_type":"code","metadata":{"id":"sI33R2gVvl2P"},"source":["import matplotlib.pyplot as plt\n","\n","cols = 10\n","rows = 2\n","\n","plt.clf()\n","fig = plt.figure(figsize=(14, 4.8))\n","for r in range(rows):\n","    for c in range(cols):\n","        ax = fig.add_subplot(r+1, cols, c+1)\n","        ax.imshow(train_data[c+r*cols][0].permute(1, 2, 0))\n","        ax.set_axis_off()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgDd3iX2zmSV"},"source":["## ネットワークモデルの定義\n","\n","畳み込みニューラルネットワークを定義します．\n","\n","ここでは，畳み込み層２層，全結合層３層から構成されるネットワークとします．\n","\n","1層目の畳み込み層は入力チャンネル数が1，出力する特徴マップ数が16，畳み込むフィルタサイズが3x3です．\n","2層目の畳み込み層は入力チャネル数が16．出力する特徴マップ数が32，畳み込むフィルタサイズは同じく3x3です．\n","１つ目の全結合層は入力ユニット数は`7*7*32`とし，出力は1024としています．\n","次の全結合層入力，出力共に1024，出力層は入力が1024，出力が10です．\n","また，活性化関数として`self.act`にシグモイド関数を定義します．\n","さらに，プーリング処理を行うための`self.pool`を定義します．\n","ここでは，maxpoolingを使用します．\n","これらの各層の構成を`__init__`関数で定義します．\n","\n","次に，`forward`関数では，定義した層を接続して処理するように記述します．\n","`forward`関数の引数`x`は入力データです．\n","それを`__init__`関数で定義した`conv1`に入力し，その出力を活性化関数である`self.act`に与えます．\n","そして，その出力を`self.pool`に与えて，プーリング処理結果を`h`として出力します．\n","2層目の畳み込み層でも同様の手順で処理を行います．\n","\n","畳み込みを適用した後の特徴マップを全結合層へと入力して，識別結果を出力します．\n","まず．畳み込みによって得られた特徴マップの形状（チャンネルx縦x横）を1次元の配列へと変換します．\n","ここで，`view()`を用いることで，`h`の配列を操作します．引数として，変換したい配列のサイズを入力します．\n","まず一つ目の引数の`h.size()[0]`で，`h`の1次元目のサイズを取得し，変換後の配列の1次元目のサイズとして指定します．\n","二つ目の引数の`-1`で任意のサイズを指定します．\n","これにより，`h`を（バッチ数x任意の長さのデータ）の形状へ変換します．\n","変換した`h`を全結合層および活性化関数へと順次入力することで，最終的にクラススコアを返します．"]},{"cell_type":"code","metadata":{"id":"TNHnp_YczmY3"},"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","        self.l1 = nn.Linear(8 * 8 * 32, 1024)\n","        self.l2 = nn.Linear(1024, 1024)\n","        self.l3 = nn.Linear(1024, 10)\n","        self.act = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","    \n","    def forward(self, x):\n","        h = self.pool(self.act(self.conv1(x)))\n","        h = self.pool(self.act(self.conv2(h)))\n","        h = h.view(h.size()[0], -1)\n","        h = self.act(self.l1(h))\n","        h = self.act(self.l2(h))\n","        h = self.l3(h)\n","        return h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Dwuvfouzmd7"},"source":["## ネットワークの作成\n","上のプログラムで定義したネットワークを作成します．\n","\n","`CNN`クラスを呼び出して，ネットワークモデルを定義します．\n","また，GPUを使う場合（`use_cuda == True`）には，ネットワークモデルをGPUメモリ上に配置します．\n","これにより，GPUを用いた演算が可能となります．\n","\n","学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．\n","また，学習率を0.01，モーメンタムを0.9として引数に与えます．"]},{"cell_type":"code","metadata":{"id":"23m79Eq-zmjl"},"source":["model = CNN()\n","if use_cuda:\n","    model.cuda()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# モデルの情報を表示\n","torchsummary.summary(model, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MUNa9Xe79vAG"},"source":["## 学習\n","読み込んだMNISTデータセットと作成したネットワークを用いて，学習を行います．\n","\n","1回の誤差を算出するデータ数（ミニバッチサイズ）を64，学習エポック数を10とします．\n","\n","次にデータローダーを定義します．\n","データローダーでは，上で読み込んだデータセット（`train_data`）を用いて，for文で指定したミニバッチサイズでデータを読み込むオブジェクトを作成します．\n","この時，`shuffle=True`と設定することで，読み込むデータを毎回ランダムに指定します．\n","\n","次に，誤差関数を設定します．\n","今回は，分類問題をあつかうため，クロスエントロピー誤差を計算するための`CrossEntropyLoss`を`criterion`として定義します．\n","\n","学習を開始します．\n","\n","各更新において，学習用データと教師データをそれぞれ`image`と`label`とします．\n","学習モデルにimageを与えて各クラスの確率yを取得します．\n","各クラスの確率yと教師ラベルtとの誤差を`criterion`で算出します．\n","また，認識精度も算出します．\n","そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．"]},{"cell_type":"code","metadata":{"id":"68RE3RTa76-W"},"source":["# ミニバッチサイズ・エポック数の設定\n","batch_size = 64\n","epoch_num = 10\n","n_iter = len(train_data) / batch_size\n","\n","# データローダーの設定\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","# 誤差関数の設定\n","criterion = nn.CrossEntropyLoss()\n","if use_cuda:\n","    criterion.cuda()\n","\n","# ネットワークを学習モードへ変更\n","model.train()\n","\n","start = time()\n","for epoch in range(1, epoch_num+1):\n","    sum_loss = 0.0\n","    count = 0\n","    \n","    for image, label in train_loader:\n","        \n","        if use_cuda:\n","            image = image.cuda()\n","            label = label.cuda()\n","            \n","        y = model(image)\n","        \n","        loss = criterion(y, label)\n","        \n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        sum_loss += loss.item()\n","        \n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","        \n","    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n","                                                                                 sum_loss / n_iter,\n","                                                                                 count.item() / len(train_loader),\n","                                                                                 time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"119eIrSmzmw6"},"source":["## テスト\n","学習したネットワークモデルを用いて評価を行います．"]},{"cell_type":"code","metadata":{"id":"yoYVMRGLzm1I"},"source":["# データローダーの準備\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n","\n","# ネットワークを評価モードへ変更\n","model.eval()\n","\n","# 評価の実行\n","count = 0\n","with torch.no_grad():\n","    for image, label in test_loader:\n","\n","        if use_cuda:\n","            image = image.cuda()\n","            label = label.cuda()\n","            \n","        y = model(image)\n","\n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","\n","print(\"test accuracy: {}\".format(count.item() / 10000.))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gzl4N5rC4j5u"},"source":["## 課題\n","\n","### 1. ネットワークの構造を変更し，認識精度の変化を確認しましょう．\n","\n","**ヒント：ネットワーク構造の変更としては，次のようなものが考えられます．**\n","* 中間層のユニット数，畳み込みのカーネルサイズ\n","* 層の数\n","* 活性化関数\n","  * `nn.Tanh()`や`nn.ReLU()`, `nn.LeakyReLU()`などが考えられます．\n","  * その他のPyTorchで使用できる活性化関数は[こちらページ](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)にまとめられています．\n","\n","\n","### 2. 学習の設定を変更し，認識精度の変化を確認しましょう．\n","\n","**ヒント：プログラムの中で変更で切る設定は次のようなものが存在します．**\n","* ミニバッチサイズ\n","* 学習回数（Epoch数）\n","* 学習率\n","* 最適化手法\n","  * `torch.optim.Adagrad()`や`torch.optim.Adam()`などが考えられます．\n","  * PyTorchで使用できる最適化手法は[こちらのページ](https://pytorch.org/docs/stable/optim.html#algorithms)にまとめられています．\n"]}]}