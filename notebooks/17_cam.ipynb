{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"17_cam.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vQfhYcVF8KPt"},"source":["# CNNの可視化 (Class Activation Mapping; CAM)\n","\n","---\n","\n","## 目的\n","Class Activation Mapping (CAM)の仕組みを理解する.\n","\n","CAMを用いてCIFAR-10データセットに対するネットワークの判断根拠の可視化を行う．\n","\n","## Class Activation Mapping (CAM)\n","Class Activation Mapping (CAM)[1]とは，ネットワークの出力に対する全結合層の結合重みを用いて，ネットワークの推論時における貢献度の高い領域をClass Activation Map (Attention map) として可視化することができる手法です．\n","CAMでは，最後の畳み込み層の後にGlobal Average Pooling (GAP)[2]を行い，特徴マップのチャネル数を全結合層のユニット数にする必要があります．\n","GAPとは，Kチャネルの特徴マップにおいて，各チャネルごとに平均値を算出し，その平均値を各特徴マップの値とする処理です．\n","これにより，全結合層におけるパラメタ数を大幅に削減することができます．\n","その後，畳み込み層で得られたKチャネルの特徴マップとクラスCに対応する全結合層\n","の結合重みを用いることで，各クラスにおけるAttention mapを獲得します．\n","<img src=\"https://www.dropbox.com/s/vjhai6vxvqgkkms/cam.png?dl=1\" width=100%>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V8ehSX6U8KPv"},"source":["## モジュールのインポート\n","プログラムの実行に必要なモジュールをインポートします．"]},{"cell_type":"code","metadata":{"id":"VaNXqInW8KPv"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.backends.cudnn as cudnn\n","import torchsummary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4gEy7cGCfz8"},"source":["## GPUの確認\n","GPUを使用した計算が可能かどうかを確認します．\n","下記のコードを実行してGPU情報を確認します． GPUの確認を行うためには，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．\n","\n","`Use CUDA: True`と表示されれば，GPUを使用した計算をPytorchで行うことが可能です． CPUとなっている場合は，上記に記載している手順にしたがって，設定を変更してください．"]},{"cell_type":"code","metadata":{"id":"p9fjeG_U8KP1"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","use_cuda = torch.cuda.is_available()\n","cudnn.benchmark = True\n","print('Use CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uanw74k0F9iw"},"source":["下記のコードを実行してGPU情報を確認します．\n","\n"]},{"cell_type":"code","metadata":{"id":"eeUIUazLGGPu"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QEgbYa9EloXV"},"source":["## 使用するデータセット\n","\n","### データセット\n","今回の物体認識では，CIFAR-10データセットを使用します．CIFAR-10データセットは，飛行機や犬などの10クラスの物体が表示されている画像から構成されたデータセットです．\n","\n","![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)"]},{"cell_type":"markdown","metadata":{"id":"xNzvYQL58KP4"},"source":["### データセットのダウンロードと読み込み\n","実験に使用するCIFAR-10データセットを読み込みます．\n","１回の誤差を算出するデータ数 (ミニバッチサイズ) は，4とします．\n","まず，CIFAR-10データセットをダウンロードします．\n","次に，ダウンロードしたデータセットを読み込みます．\n","学習には，大量のデータを利用しますが，それでも十分ではありません． そこで，データ拡張 (data augmentation) により，データのバリエーションを増やします． 一般的な方法は，画像の左右反転，明るさ変換などです． また，画像の一部にマスク処理をかけるRandom Erasingも行います．"]},{"cell_type":"code","metadata":{"id":"leWJTOIL8KP4"},"source":["batch_size = 4\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    transforms.RandomErasing(),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=20)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=20)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zHQfAwb_8KP7"},"source":["### CIFAR-10データセットの表示\n","CIFAR-10データセットに含まれる画像を表示してみます．\n","ここでは，matplotlibを用いて複数の画像を表示させるプログラムを利用します．\n","学習データは5万枚，1つのデータサイズは3x32x32の画像のような形式となっています． これは32x32ピクセルのカラー画像という意味になります．\n","\n"]},{"cell_type":"code","metadata":{"id":"Fod_SRFR8KP8"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# functions to show an image\n","def imshow(img):\n","    npimg = img.numpy()\n","    npimg = ((npimg.transpose((1,2,0))  * [0.2023, 0.1994, 0.2010]) + [0.4914, 0.4822, 0.4465])  # unnormalize\n","    plt.imshow(npimg)\n","    plt.show()\n","\n","# get some random training images\n","dataiter = iter(testloader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images[0:4]))\n","# print labels\n","print(' '.join('%5s' % classes[labels[j]] for j in range(2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DduOi-s18KP_"},"source":["## ネットワークモデルの定義\n","次に，CAMを定義します．\n","今回は，CNNにCAMを導入したモデル (CAM) を使用します．"]},{"cell_type":"code","metadata":{"id":"O0cpWP_w8KP_"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","class CAM(nn.Module):\n","    def __init__(self):\n","        super(CAM, self).__init__()\n","        num_classes = 10\n","\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv2d(32, 128, kernel_size=3, padding=1)\n","        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","        self.gap = nn.AvgPool2d(16)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = self.relu(self.conv3(x))\n","        x = self.relu(self.conv4(x))\n","        x = self.relu(self.conv5(x))\n","\n","        self.attmap = x\n","        x = self.gap(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x, [self.attmap, self.fc.weight]\n","\n","model = CAM()\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uFCekfs78KQC"},"source":["## 損失関数と最適化手法の定義\n","学習に使用する損失関数と最適化手法を定義します．\n","各更新において，学習用データと教師データをそれぞれ`inputs`と`targets`とします．\n","学習モデルに`inputs`を与えて，ネットワークの出力とAttention mapを取得します．\n","ネットワークの出力は教師ラベル`targets`との誤差を`criterion`で算出します．\n","また，認識精度も算出します．\n","そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n","最適化手法には，Adamを用いて学習を行います．\n","\n","最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．"]},{"cell_type":"code","metadata":{"id":"rntVJhx98KQC"},"source":["import torch.optim as optim\n","from torch.optim import SGD, lr_scheduler\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9,  weight_decay=5e-4)\n","\n","# モデルの情報を表示\n","torchsummary.summary(model, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4NRFd6i8KQF"},"source":["## 学習\n","学習エポック数を20とします．\n","CIFAR-10データセットの学習データサイズを取得し，1エポック内における更新回数を求めます．\n","1エポック学習するごとに学習したモデルを評価し，最も精度の高いモデルが保存されます．"]},{"cell_type":"code","metadata":{"id":"nnMVTad98KQG"},"source":["epochs = 20\n","best_acc = 0  # best test accuracy\n","\n","for epoch in range(epochs):\n","    train_running_loss = 0.0\n","    train_running_acc = 0.0\n","\n","    # training\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    count = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs, _ = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        # print statistics\n","        train_running_loss += loss\n","        train_running_acc += 100.*correct/total\n","        count += 1\n","\n","    print('[Epoch %d] Train Loss: %.5f | Train Acc: %.3f%%'\n","                  % (epoch + 1, train_loss/count, train_running_acc/count))\n","\n","    # testing\n","    model.eval() \n","    with torch.no_grad():\n","        test_running_loss = 0.0\n","        test_running_acc = 0.0\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        count = 0\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs, _ = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            # print statistics\n","            test_running_loss += loss\n","            test_running_acc += 100.*correct/total\n","            count += 1\n","\n","        print('Test Loss: %.5f | Test Acc: %.3f%%'\n","                      % (test_loss/count, test_running_acc/count))\n","        \n","    # save model\n","    if test_running_acc/count > best_acc:\n","        best_acc = max(test_running_acc/count, best_acc)\n","        PATH = './cifar_net.pth'\n","        torch.save(model.state_dict(), PATH)\n","    \n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlOo-rzY8KQL"},"source":["##テスト\n","学習したネットワークのテストデータに対する認識率の確認を行います．\n","まず，学習したネットワークを評価するために保存したモデルをロードします．"]},{"cell_type":"code","metadata":{"id":"WSa0ATFj8KQP"},"source":["model = CAM()\n","model.to(device)\n","PATH = './cifar_net.pth'\n","model.load_state_dict(torch.load(PATH))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VwWfJO358KQS"},"source":["次に，学習したネットワークを用いて，テストデータに対する認識率の確認を行います．\n","`model.eval()`を適用することで，ネットワーク演算を評価モードへ変更します． これにより，学習時と評価時で挙動が異なる演算（dropout等）を変更することが可能です． また，`torch.no_grad()`を適用することで，学習時には必要になる勾配情報を保持することなく演算を行います．"]},{"cell_type":"code","metadata":{"id":"ij38kQBP8KQS"},"source":["model.eval()\n","\n","running_loss = 0.0\n","running_acc = 0.0\n","test_loss = 0\n","correct = 0\n","total = 0\n","count = 0\n","\n","with torch.no_grad():\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        outputs, _ = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        test_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","        # print statistics\n","        running_loss += loss\n","        running_acc += 100.*correct/total\n","        count += 1\n","\n","    print('Test Loss: %.5f | Test Acc: %.3f%%'\n","                  % (test_loss/count, running_acc/count))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qiY_kiXb8KQV"},"source":["## CAMによるAttention mapの可視化\n","上位5クラスに対するAttention mapを可視化して，ネットワークの判断根拠を確認してみます．\n","再度，実行することで他のテストサンプルに対するAttention mapを可視化することができます．\n","pred (prediction) は認識結果，conf (confidence) は認識結果に対する信頼度を示しています．"]},{"cell_type":"code","metadata":{"id":"cB0SvioEwlm4"},"source":["import cv2\n","from ipykernel import kernelapp as app\n","\n","testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n","                                         shuffle=True, num_workers=20)\n","\n","classes_list = ['plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","softmax = nn.Softmax(dim=1)\n","\n","def min_max(x, axis=None):\n","    x_min = x.min(axis=axis, keepdims=True)\n","    x_max = x.max(axis=axis, keepdims=True)\n","    return (x - x_min) / (x_max - x_min)\n","\n","with torch.no_grad():\n","    v_list = []\n","    att_list = []\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        outputs, attention = model(inputs)\n","        outputs = softmax(outputs)\n","        d_inputs = inputs.data.cpu()\n","        d_inputs = d_inputs.numpy()\n","\n","        in_b, in_c, in_y, in_x = inputs.shape\n","        item_img = d_inputs[0]\n","        v_img = ((item_img.transpose((1, 2, 0)) * [0.2023, 0.1994, 0.2010]) + [0.4914, 0.4822, 0.4465]) * 255\n","        v_img = np.uint8(v_img)\n","\n","        # Class Activation Mapping\n","        out_put = outputs.data.topk(5, 1, True, True)\n","        top5_score, top5_idx = out_put\n","        top5_idx = top5_idx.cpu()\n","        top5_idx = top5_idx.numpy()\n","        maps, vecs = attention\n","\n","        for idx_rank, item_rank in enumerate(top5_idx[0]):\n","            contributes = vecs[item_rank]\n","            vis_maps = maps[0] * contributes.reshape(contributes.shape[0], 1, 1)\n","            vis_maps = vis_maps.data.cpu()\n","            vis_maps = vis_maps.numpy()\n","            sum_map = np.sum(vis_maps, axis=0)\n","\n","            resize_map = cv2.resize(sum_map, (in_x, in_y))\n","            resize_map = min_max(resize_map)\n","            resize_map *= 255.\n","            resize_map = np.uint8(resize_map)\n","\n","            jet_map = cv2.applyColorMap(resize_map, cv2.COLORMAP_JET)\n","            jet_map = cv2.addWeighted(v_img, 0.6, jet_map, 0.4, 0)\n","            v_list.append(v_img)\n","            att_list.append(jet_map)\n","        break\n","\n","# Show attention map\n","cols = 1\n","rows = 1\n","\n","fig = plt.figure(figsize=(14, 2.0))\n","plt.axis(\"off\")\n","for r in range(rows):\n","    for c in range(cols):\n","        cls = targets[c].item()\n","        ax = fig.add_subplot(r+1, cols, c+1)\n","        plt.title('Input image\\n\\n{}'.format(classes_list[cls]))\n","        ax.imshow(v_list[cols * r + c])\n","        ax.set_axis_off()\n","plt.show()\n","\n","a_cols = 5\n","a_rows = 1\n","\n","fig = plt.figure(figsize=(12, 1.5))\n","plt.title('Attention map\\n\\n\\n')\n","plt.axis(\"off\")\n","for r in range(a_rows):\n","    for c in range(a_cols):\n","        cls = top5_idx[0][c].item()\n","        conf = top5_score[0][c]\n","        ax = fig.add_subplot(r+1, a_cols, c+1)\n","        plt.title('class: {}\\nconf: {:.2f}'.format(classes_list[cls], conf))\n","        ax.imshow(att_list[a_cols * r + c])\n","        ax.set_axis_off()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFFp16Bgcs4x"},"source":["#課題\n","1. ハイパーパラメータ (学習率，エポック数，ミニバッチサイズ等) を変更して，Attention mapの変化を確認してみましょう．"]},{"cell_type":"code","metadata":{"id":"0XznUZILdgos"},"source":["#ここにコードを書く"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MjKIGtvknT-G"},"source":["# 参考文献\n","- [1] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva,\n","and A. Torralba, \"Learning deep features for discriminative\n","localization\". In 2016 IEEE Conference on Computer\n","Vision and Pattern Recognition, pp. 2921–2929, 2016.\n","\n","- [2] M. Lin, Q. Chen, and S. Yan, \"Network in network\".\n","In 2nd International Conference on Learning Representations,\n","Banff, AB, Canada, April 14-16, 2014, Conference\n","Track Proceedings, 2014."]}]}